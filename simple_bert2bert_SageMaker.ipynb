{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = os.environ.get('SM_HP_MODEL_TYPE', 'bert-base-uncased')\n",
    "epochs = int(os.environ.get('SM_HP_EPOCHS', 1))\n",
    "batch = int(os.environ.get('SM_HP_BATCH', 4))\n",
    "lr = float(os.environ.get('SM_HP_LR', 1e-5))\n",
    "\n",
    "train_remotely = bool(int(os.environ.get('SM_HP_TRAIN_REMOTELY', 0)))  # Should be False for local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import AdamW\n",
    "from transformers import (EncoderDecoderModel,\n",
    "                          BertTokenizerFast,\n",
    "                          BertGenerationEncoder,\n",
    "                          BertGenerationDecoder)\n",
    "from typing import Callable\n",
    "from loguru import logger\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_type)\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_checkpoint_name):\n",
    "    encoder = BertGenerationEncoder.from_pretrained(model_checkpoint_name,\n",
    "                                                    bos_token_id=tokenizer.bos_token,\n",
    "                                                    eos_token_id=tokenizer.eos_token)\n",
    "    \n",
    "    decoder = BertGenerationDecoder.from_pretrained(model_checkpoint_name,\n",
    "                                                    add_cross_attention=True,\n",
    "                                                    is_decoder=True,\n",
    "                                                    bos_token_id=tokenizer.bos_token,\n",
    "                                                    eos_token_id=tokenizer.eos_token)\n",
    "    decoder.bert.encoder.requires_grad_(True)\n",
    "    decoder.lm_head.requires_grad_(True)\n",
    "    decoder.bert.embeddings.requires_grad_(False)\n",
    "\n",
    "    encoder.requires_grad_(False)\n",
    "\n",
    "    model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length = 512\n",
    "decoder_max_length = 128\n",
    "\n",
    "def run_epoch(model: nn.Module,\n",
    "              data_loader: DataLoader,\n",
    "              tokenizer: BertTokenizerFast,\n",
    "              post_hook: Callable = None):\n",
    "    \n",
    "    loss = 0\n",
    "    num_batches = len(data_loader)\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        input_ids = tokenizer(batch[\"article\"], \n",
    "                              padding=\"max_length\", \n",
    "                              truncation=True, \n",
    "                              max_length=encoder_max_length,\n",
    "                              return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        output_ids = tokenizer(batch[\"highlights\"], \n",
    "                               padding=\"max_length\", \n",
    "                               truncation=True, \n",
    "                               max_length=decoder_max_length,\n",
    "                               return_tensors=\"pt\").input_ids\n",
    "\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        decoder_input_ids=output_ids,\n",
    "                        labels=output_ids,\n",
    "                        return_dict=True)\n",
    "        batch_loss = outputs.loss.sum()\n",
    "        loss += batch_loss.item()\n",
    "        \n",
    "        if post_hook is not None:\n",
    "            post_hook(i, num_batches, batch_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train(epochs: int,\n",
    "          lr: float,\n",
    "          train_data_loader: DataLoader,\n",
    "          valid_data_loader: DataLoader = None,\n",
    "          rank = None):\n",
    "    model = create_model(model_type)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_type)\n",
    "\n",
    "    def update_weights_hook(bi, num_batches, batch_loss):\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pct10 = math.ceil(num_batches / 10)\n",
    "        if bi % pct10 == 0 or bi == num_batches-1:\n",
    "            logger.info(f'training: batch={bi+1}/{num_batches}; batch_error={batch_loss.item():.5f};')\n",
    "                  \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = run_epoch(model, train_data_loader, tokenizer, update_weights_hook)\n",
    "\n",
    "        if valid_data_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_loss = run_epoch(model, valid_data_loader, tokenizer)\n",
    "        else:\n",
    "            val_loss = 'N/A'\n",
    "\n",
    "        logger.info(f'epoch={i}; train_error={train_loss:.5f};  valid_error={val_loss:.5f};')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(\n",
    "    datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\").select(range(32)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "valid_set = DataLoader(\n",
    "    datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\").select(range(12)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "if not train_remotely:\n",
    "    model = train(epochs=1, lr=lr, train_data_loader=train_set, valid_data_loader=valid_set)\n",
    "elif:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    output_path = f's3://chegg-ds-data/oboiko/bert_demo'\n",
    "\n",
    "    pytorch_estimator = PyTorch(entry_point='train.sh',\n",
    "                                base_job_name='bert_demo',\n",
    "                                role=role,\n",
    "                                train_instance_count=1,\n",
    "                                train_instance_type='ml.p2.xlarge',  # GPU instance\n",
    "                                train_volume_size=50,\n",
    "                                train_max_run=86400,  # 24 hours\n",
    "                                hyperparameters={\n",
    "                                  'model_type': 'bert-base-uncased',\n",
    "                                  'batch': 32,\n",
    "                                  'epochs': 10,\n",
    "                                  'lr': 1e-5,\n",
    "                                    \n",
    "                                  'train_remotely': 0,\n",
    "                                  'notebook_name': 'simple_bert2bert_SageMaker'  # Inconvenient and error prone >:(\n",
    "                                },\n",
    "                                framework_version='1.6.0',\n",
    "                                py_version='py3',\n",
    "                                source_dir='.',  # This entire folder will be transferred to training instance\n",
    "                                debugger_hook_config=False,\n",
    "                                output_path=output_path,  # Model files will be uploaded here\n",
    "                                image_name='954558792927.dkr.ecr.us-west-2.amazonaws.com/sagemaker/wdm:latest',\n",
    "                                metric_definitions=[\n",
    "                                    {'Name': 'train:error', 'Regex': 'train_error=(.*?);'},\n",
    "                                    {'Name': 'validation:error', 'Regex': 'valid_error=(.*?);'},\n",
    "                                    {'Name': 'batch:error', 'Regex': 'batch_error=(.*?);'}\n",
    "                                ]\n",
    "                     )\n",
    "    pytorch_estimator.fit('s3://chegg-ds-data/oboiko/wdm/dummy.txt', wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
